# -*- coding: utf-8 -*-
"""Sentiment Analysis of Arabic and Turkish languages.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X0d-0m8o_5vQuTYfno2MSHom384kviFl
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --upgrade sklearn
# !pip install Arabic-Stopwords
# !pip install emoji
# !pip install Tashaphyne
# !pip install qalsadi
# !pip install langdetect
# !pip install --upgrade scikit-learn
# !pip install nltk
# !pip install pandas
# !pip install seaborn
# !pip install wordcloud
# !pip install arabic_reshaper
# !pip install python-bidi
# !pip install transformers
# !pip install tensorflow
# !pip install tf-keras
# !pip install scipy

"""# **Importing Libraries**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re

# Load your Arabic and Turkish datasets
# Replace 'arabic_data.csv' and 'turkish_data.csv' with the paths to your datasets
arabic_df = pd.read_csv('CompanyReviews.csv')
turkish_df = pd.read_csv('TurkishReviews.csv')

# Changing the Turkish comments' colmumn name as the Arabic comments column' name
turkish_df = turkish_df.rename(columns={'Yorum': 'review_description', 'Duygu': 'rating'})

# Add a new column to each dataset to identify the language
arabic_df['language'] = 'Arabic'
turkish_df['language'] = 'Turkish'

# Concatenate the two datasets
df = pd.concat([arabic_df, turkish_df])

df

"""# **Data Cleaning**"""

# Drop the first column
df = df.drop(df.columns[0], axis=1)

# Replace the rating values
df['rating'] = df['rating'].replace({1: 'positive', -1: 'negative', 0: 'neutral', 'Olumlu': 'positive' , 'Olumsuz': 'negative', 'Tarafsiz': 'neutral'})
df

# Replace NaN values in 'company' column with 'n/a'
df['company'] = df['company'].fillna('n/a')

#display the counts of each category
df['rating'].value_counts()

# Define the valid ratings
valid_ratings = ['negative', 'positive', 'neutral']

# Keep only the rows in your DataFrame where the 'rating' column is in valid_ratings
df = df[df['rating'].isin(valid_ratings)]

#display the counts of each category
df['rating'].value_counts()

# Group the DataFrame 'df' by the 'company' column and calculate the size of each group
# This will return a Series with the number of occurrences for each unique 'company' value
df.groupby('company').size()

# display the sum of null values in each column
df.isnull().sum()

# dropping rows that has null values in review_description column
df = df.dropna(subset=['review_description'])
df.isnull().sum()

# display the sum of duplicates in reviews
df.review_description.duplicated().sum()

# deleting rows that has the same reviews
df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)
df.review_description.duplicated().sum()

# Reset the index of the DataFrame
df = df.reset_index(drop=True)
df

"""## **Visualization**"""

# Count the number of occurrences of each rating
rating_counts = df['rating'].value_counts()

# Define a color for each rating
colors = ['green' if label == 'positive'
          else 'red' if label == 'negative'
          else 'gray'
          for label in rating_counts.index]

# Create the pie chart
plt.figure(figsize=(10,6))
plt.pie(rating_counts, labels = rating_counts.index, autopct='%1.1f%%', colors=colors)
plt.title('Ratings')
plt.show()

# Assuming 'language' is the name of the column with language labels
# and 'rating' is the name of the column with rating labels

# Filter the DataFrame for Turkish language reviews and count the ratings
turkish_ratings = df[df['language'] == 'Turkish']['rating'].value_counts()

# Define a color for each Turkish rating
turkish_colors = ['green' if label == 'positive'
                  else 'red' if label == 'negative'
                  else 'gray'
                  for label in turkish_ratings.index]

# Create the pie chart for Turkish ratings
plt.figure(figsize=(10,6))
plt.pie(turkish_ratings, labels = turkish_ratings.index, autopct='%1.1f%%', colors=turkish_colors)
plt.title('Turkish Ratings')
plt.show()

# Filter the DataFrame for Arabic language reviews and count the ratings
arabic_ratings = df[df['language'] == 'Arabic']['rating'].value_counts()

# Define a color for each Arabic rating
arabic_colors = ['green' if label == 'positive'
                 else 'red' if label == 'negative'
                 else 'gray'
                 for label in arabic_ratings.index]

# Create the pie chart for Arabic ratings
plt.figure(figsize=(10,6))
plt.pie(arabic_ratings, labels = arabic_ratings.index, autopct='%1.1f%%', colors=arabic_colors)
plt.title('Arabic Ratings')
plt.show()

# Get the value counts
rating_counts = df['rating'].value_counts()

# Define a color for each rating
colors = ['green' if label == 'positive'
          else 'red' if label == 'negative'
          else 'gray'
          for label in rating_counts.index]

# Create the bar plot
rating_counts.plot(kind='bar', color=colors)
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.title('Bar Plot of Ratings')
plt.show()

df.company.unique()

"""# **Data Preprocessing**"""

df.review_description=df.review_description.astype(str)
df.review_description=df.review_description.apply(lambda x:re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', x))
df.review_description=df.review_description.apply(lambda x:x.replace('؛',"", ))
df

# Load the stop words with the correct encoding
with open('arabic.txt', 'r', encoding='utf-8') as f:
    arabic_stopwords = f.read().splitlines()

with open('turkish.txt', 'r', encoding='utf-8') as f:
    turkish_stopwords = f.read().splitlines()

#Some words needed to work with to will remove
for word in ['لا','لكن']:
    arabic_stopwords.remove(word)

# Combine the stop words into one list
all_stopwords = arabic_stopwords + turkish_stopwords

df.review_description = df.review_description.apply(lambda x:" ".join([word for word in x.split() if word not in all_stopwords]))

df

# Handling Emojis in reviews
arabic_emojis = {
    "🙂":"يبتسم",
    "😂":"يضحك",
    "💔":"قلب حزين",
    "🙂":"يبتسم",
    "❤️":"حب",
    "❤":"حب",
    "😍":"حب",
    "😭":"يبكي",
    "😢":"حزن",
    "😔":"حزن",
    "♥":"حب",
    "💜":"حب",
    "😅":"يضحك",
    "🙁":"حزين",
    "💕":"حب",
    "💙":"حب",
    "😞":"حزين",
    "😊":"سعادة",
    "👏":"يصفق",
    "👌":"احسنت",
    "😴":"ينام",
    "😀":"يضحك",
    "😌":"حزين",
    "🌹":"وردة",
    "🙈":"حب",
    "😄":"يضحك",
    "😐":"محايد",
    "✌":"منتصر",
    "✨":"نجمه",
    "🤔":"تفكير",
    "😏":"يستهزء",
    "😒":"يستهزء",
    "🙄":"ملل",
    "😕":"عصبية",
    "😃":"يضحك",
    "🌸":"وردة",
    "😓":"حزن",
    "💞":"حب",
    "💗":"حب",
    "😑":"منزعج",
    "😎":"ثقة",
    "💛":"حب",
    "😩":"حزين",
    "💪":"عضلات",
    "👍":"موافق",
    "🙏🏻":"رجاء طلب",
    "😳":"مصدوم",
    "👏🏼":"تصفيق",
    "🌚":"صمت",
    "💚":"حب",
    "🙏":"رجاء طلب",
    "💘":"حب",
    "☺":"يضحك",
    "😶":"مصدوم",
    "✌️":"مرح",
    "✋🏻":"توقف",
    "😉":"غمزة",
    "🌷":"حب",
    "🙃":"مبتسم",
    "😫":"حزين",
    "😨":"مصدوم",
    "💟":"حب",
    "😪":"حزن",
    "😆":"يضحك",
    "😣":"استياء",
    "☺️":"حب",
    "😱":"كارثة",
    "😁":"يضحك",
    "😖":"استياء",
    "🏃🏼":"يجري",
    "😡":"غضب",
    "🚶":"يسير",
    "🤕":"مرض",
    "‼️":"تعجب",
    "👌🏻":"احسنت",
    "❣":"حب",
    "🙊":"مصدوم",
    "💃":"سعادة مرح",
    "💃🏼":"سعادة مرح",
    "😜":"مرح",
    "👊":"ضربة",
    "😟":"استياء",
    "💖":"حب",
    "😥":"حزن",
    "💎":"الماظ",
    "😷":"وباء مرض",
    "⚠️" :"تحذير",
    "🤗" : "احتواء",
    "✖️": "غلط",
    "👑" : "تاج",
    "✔️" : "صح",
    "💌": "قلب",
    "😲" : "مندهش",
    "🚫" : "خطا",
    "👏🏻" : "برافو",
    "👍🏻": "تمام",
    "✌🏼": "علامه النصر",
    "🌝":"مبتسم",
    "😧" : "قلق و صدمة",
    "❗️" :"تعجب",
    "👐🏽":"ايدي مفتوحه",
    "👌🏽": "بالظبط",
    "⁉️" : "استنكار",
    "🎀":    "ورده",
    "💵":  "فلوس",
    "😋":  "جائع",
    "😛":  "يغيظ",
    "😠":  "غاضب",
    "❌":"رفض",
    "👌🏼":"احسنت",
    "😮":"صدمة",
    "😦":"قلق",
    "🐤":"حزن",
    "💫":"مرح",
    "💝":"حب",
    "❤︎":"حب",
    "🙏🏼":"رجاء طلب",
    "‼":"تعجب",
    "♥️":"حب",
    "💩":"معترض",
    "🌟":"نجمة",
    "👊🏼":"ضرب",
    "👊":"ضربة",
    "😚":"حب",
    "👎🏻":"لا يعجبني",
    "👊🏽":"ضربة",
    "😙":"حب",
    "👏🏽":"يصفق",
    "💪🏻":"عضلات",
    "🔥":"حريق",
    "😬":"عدم الراحة",
    "👊🏿":"يضرب",
    "✋🏼":"كف ايد",
    "☠️":"وجه مرعب",
    "🎉":"يهنئ",
    "😿":"وجه حزين",
    "☹️":"وجه يائس",
    "😘" :"حب",
    "😰" :"خوف و حزن",
    "🌼":"ورده",
    "💋":  "بوسه",
    "❣️":"حب",
    "😇":"دايخ",
    "😈":"رعب",
    "❗️":"تعجب",
    "👎":"غير موافق",
    "😯":"متفاجأ",
    "😻":"اعجاب",
    "💪🏽":"قوه",
    "😤":"وجه عابس",
    "😹":"ضحك",
    "💓":"حب",
    "👻":"رعب",
    "❎":"خطء",
    "🤮":"حزن",
    }

turkish_emojis = {
    "🙂": "Gülümseme",
    "😂": "Kahkaha",
    "💔": "Kırık Kalp",
    "❤️": "Aşk",
    "😍": "Aşık Olmuş",
    "😭": "Ağlama",
    "😢": "Üzgün",
    "😔": "Düşünceli",
    "♥": "Aşk",
    "💜": "Aşk",
    "😅": "Terli Gülümseme",
    "🙁": "Üzgün",
    "💕": "Aşk",
    "💙": "Aşk",
    "😞": "Üzgün",
    "😊": "Mutluluk",
    "👏": "Alkış",
    "👌": "Mükemmel",
    "😴": "Uyku",
    "😀": "Kahkaha",
    "😌": "Rahatlamış",
    "🌹": "Gül",
    "🙈": "Utangaç",
    "😄": "Kahkaha",
    "😐": "Nötr",
    "✌": "Zafer",
    "✨": "Parıltı",
    "🤔": "Düşünce",
    "😏": "Küstah",
    "😒": "Rahatsız",
    "🙄": "Sıkılmış",
    "😕": "Hayal Kırıklığı",
    "😃": "Kahkaha",
    "🌸": "Çiçek",
    "😓": "Üzgün",
    "💞": "Aşk",
    "💗": "Aşk",
    "😑": "Rahatsız",
    "😎": "Havalı",
    "💛": "Aşk",
    "😩": "Yorgun",
    "💪": "Güç",
    "👍": "Onay",
    "🙏🏻": "Dua",
    "😳": "Şaşkın",
    "👏🏼": "Alkış",
    "🌚": "Gizemli",
    "💚": "Aşk",
    "🙏": "Dua",
    "💘": "Aşk",
    "☺": "Gülümseme",
    "😶": "Şaşkın",
    "✌️": "Eğlence",
    "✋🏻": "Dur",
    "😉": "Göz Kırpma",
    "🌷": "Çiçek",
    "🙃": "Ters Gülümseme",
    "😫": "Yorgun",
    "😨": "Korkmuş",
    "💟": "Aşk",
    "😪": "Uykulu",
    "😆": "Kahkaha",
    "😣": "Üzgün",
    "☺️": "Gülümseme",
    "😱": "Panik",
    "😁": "Kahkaha",
    "😖": "Rahatsız",
    "🏃🏼": "Koşu",
    "😡": "Öfkeli",
    "🚶": "Yürüme",
    "🤕": "Yaralı",
    "‼️": "Ünlem",
    "👌🏻": "Mükemmel",
    "❣": "Aşk",
    "🙊": "Sessiz",
    "💃": "Dans",
    "💃🏼": "Dans",
    "😜": "Şakacı",
    "👊": "Yumruk",
    "😟": "Endişeli",
    "💖": "Aşk",
    "😥": "Üzgün",
    "💎": "Elmas",
    "😷": "Hasta",
    "⚠️": "Uyarı",
    "🤗": "Sarılmak",
    "✖️": "Yanlış",
    "👑": "Taç",
    "✔️": "Doğru",
    "💌": "Mektup",
    "😲": "Şaşkın",
    "🚫": "Yasak",
    "👏🏻": "Alkış",
    "👍🏻": "Onay",
    "✌🏼": "Zafer İşareti",
    "🌝": "Gülümseme",
    "😧": "Endişeli",
    "❗️": "Ünlem",
    "👐🏽": "Açık Eller",
    "👌🏽": "Tamam",
    "⁉️": "Soru Ünlem",
    "🎀": "Kurdele",
    "💵": "Para",
    "😋": "Lezzetli",
    "😛": "Dil Çıkarma",
    "😠": "Kızgın",
    "❌": "Yanlış",
    "👌🏼": "Mükemmel",
    "😮": "Şaşkın",
    "😦": "Endişeli",
    "🐤": "Kuş",
    "💫": "Baş Dönmesi",
    "💝": "Hediye Kalbi",
    "❤︎": "Aşk",
    "🙏🏼": "Dua",
    "‼": "Ünlem",
    "♥️": "Aşk",
    "💩": "Bok",
    "🌟": "Yıldız",
    "👊🏼": "Yumruk",
    "😚": "Öpücük",
    "👎🏻": "Beğenmeme",
    "👊🏽": "Yumruk",
    "😙": "Öpücük",
    "👏🏽": "Alkış",
    "💪🏻": "Güç",
    "🔥": "Ateş",
    "😬": "Sıkıntı",
    "👊🏿": "Yumruk",
    "✋🏼": "El",
    "☠️": "Kafatası",
    "🎉": "Kutlama",
    "😿": "Ağlayan Kedi",
    "☹️": "Üzgün Yüz",
    "😘": "Öpücük",
    "😰": "Endişeli",
    "🌼": "Çiçek",
    "💋": "Öpücük",
    "❣️": "Aşk",
    "😇": "Masum",
    "😈": "Şeytan",
    "👎": "Beğenmeme",
    "😯": "Şaşkın",
    "😻": "Kalp Gözlü Kedi",
    "💪🏽": "Güç",
    "😤": "Buğulu Nefes",
    "😹": "Gülen Kedi",
    "💓": "Kalp Atışı",
    "👻": "Hayalet",
    "❎": "Yanlış",
    "🤮": "Kusma",
}


emotions_to_emoji = {
    ":)" : "🙂",
    ":(" : "🙁",
    "xD" : "😆",
    ":=(": "😭",
    ":'(": "😢",
    ":'‑(": "😢",
    "XD" : "😂",
    ":D" : "🙂",
    "♬" : "🎵",
    "♡" : "❤️",
    "☻"  : "🙂",
}

from langdetect import detect, LangDetectException
import emoji

def emoticons_to_emoji(text):
    for emoticon, emoji in emotions_to_emoji.items():
        text = text.replace(emoticon, emoji)
    return text

def checkemojie(text, lang):
    emojistext=[]
    for char in text:
        if any(emoji.distinct_emoji_list(char)):
            if lang == 'ar':
                if char in arabic_emojis.keys():
                    emojistext.append(arabic_emojis[emoji.distinct_emoji_list(char)[0]])
            else:
                if char in turkish_emojis.keys():
                    emojistext.append(turkish_emojis[emoji.distinct_emoji_list(char)[0]])
    return " ".join(emojistext)

def emojiTextTransform(text):
    text = emoticons_to_emoji(text)  # Transform emoticons to emojis
    cleantext = re.sub(r'[^\w\s]', '', text)
    try:
        lang = detect(cleantext)  # Detect the language of the text
    except LangDetectException:
        lang = 'unknown'
    return cleantext + " " + checkemojie(text, lang)




emojiTextTransform(df.review_description[71])

emojiTextTransform(df.review_description[38999])

df.review_description=df.review_description.apply(lambda x:emojiTextTransform(x))
df.review_description[71]

df.review_description.duplicated().sum()

df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)

# remove digits
df.review_description=df.review_description.apply(lambda x:''.join([word for word in x if not word.isdigit()]))

#applying lemmatizer
import qalsadi.lemmatizer
lemmer = qalsadi.lemmatizer.Lemmatizer()

df.review_description=df.review_description.apply(lambda x:lemmer.lemmatize_text(x))
df

df.review_description=df.review_description.apply(lambda x:" ".join(x))
df

df.review_description.duplicated().sum()

df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)

df.review_description.duplicated().sum()

df.review_description.isnull().sum()

import arabic_reshaper
from bidi.algorithm import get_display

# Function to reshape and apply bidi support to Arabic text
def reshape_arabic_text(text):
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    return bidi_text

# Apply the reshaping function to the Arabic text only
arabic_reviews = ' '.join(df[df['language'] == 'Arabic']['review_description'])
reshaped_arabic_reviews = reshape_arabic_text(arabic_reviews)

from wordcloud import WordCloud
# Generate the word cloud using an Arabic font
wordcloud = WordCloud(
    width = 800, height = 800,
    background_color ='white',
    stopwords = all_stopwords,
    min_font_size = 10,
    font_path=r'C:\Users\Tameem\Arial Unicode MS Regular.ttf'  # Specify the path to an Arabic font
).generate(reshaped_arabic_reviews)

# Display the word cloud using matplotlib
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

from scipy.sparse import hstack
import numpy as np
# Feature Extraction
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming 'df' is your DataFrame and it has a 'language' column to distinguish between Turkish and Arabic
turkish_texts = df[df['language'] == 'Turkish']['review_description']
arabic_texts = df[df['language'] == 'Arabic']['review_description']

# Vectorize Turkish texts
turkish_vectorizer = TfidfVectorizer()
turkish_features = turkish_vectorizer.fit_transform(turkish_texts)

# Optionally, vectorize Arabic texts
arabic_vectorizer = TfidfVectorizer(max_features=10000)
arabic_features = arabic_vectorizer.fit_transform(arabic_texts)

# Now, 'turkish_features' contains the TF-IDF features for Turkish texts,
# and 'arabic_features' contains the TF-IDF features for Arabic texts.


# for Turkish and Arabic texts respectively
turkish_feature_names = turkish_vectorizer.get_feature_names_out()
arabic_feature_names = arabic_vectorizer.get_feature_names_out()

def label_to_num(label):
    return {'negative': -1, 'neutral': 0, 'positive': 1}[label]

df['rating'] = df['rating'].apply(label_to_num)  # Convert string labels to numerical values

# remap the class labels in the ‘rating’ column of the DataFrame, it maps -1 to 0, 0 to 1, and 1 to 2
def mappingclasses(classx):
    return {-1:0,0:1,1:2}[classx]
df['rating']=df['rating'].apply(lambda x:mappingclasses(x))

# Create a DataFrame with the feature names as columns
X1 = pd.DataFrame(arabic_features.toarray(),columns=arabic_feature_names)
X2= pd.DataFrame(turkish_features.toarray(),columns=turkish_feature_names)

Y=df.rating

from sklearn.model_selection import train_test_split
# Assuming 'df' has a 'rating' column with labels and a 'language' column to distinguish languages
arabic_labels = df[df['language'] == 'Arabic']['rating']
turkish_labels = df[df['language'] == 'Turkish']['rating']

# Split the Arabic data
x1_train, x1_test, y1_train, y1_test = train_test_split(X1, arabic_labels, random_state=42, test_size=0.20, shuffle=True)

# Split the Turkish data
x2_train, x2_test, y2_train, y2_test = train_test_split(X2, turkish_labels, random_state=42, test_size=0.20, shuffle=True)

"""# **Modelling**"""

import numpy as np
from sklearn import metrics
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, classification_report , roc_curve, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score,make_scorer,mean_squared_error
from sklearn.model_selection import cross_val_score, GridSearchCV


def get_accuracy(name, trained_model , x_train, y_train, x_test, y_test):
    tree_predict = trained_model.predict(x_test)
    print("Testing accuracy   :",metrics.accuracy_score(y_test, tree_predict)*100 , "%")
    tree_predict1 = trained_model.predict(x_train)
    print("Training accuracy  :",metrics.accuracy_score(y_train, tree_predict1)*100 ,"%")
    print("precision : ",precision_score(y_test, tree_predict,average='micro'))
    print("recall    : ",recall_score(y_test, tree_predict,average='micro'))
    print("f1_score  : ",f1_score(y_test, tree_predict,average='micro'))

    cf1 = confusion_matrix(y_test,tree_predict)
    sns.heatmap(cf1,annot=True,fmt = '.0f')
    plt.xlabel('prediction')
    plt.ylabel('Actual')
    plt.title(name+ ' Confusion Matrix')
    plt.show()

    print("                      Traing Classification Report                      ")
    print(classification_report(y_train,  trained_model.predict(x_train)))
    print("                      Testing Classification Report                      ")
    print(classification_report(y_test,  trained_model.predict(x_test)))

"""# **Random Forrest**"""

from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
trained_clf_random_forest1 = RandomForestClassifier().fit(x1_train, y1_train)
get_accuracy('RandomForestClassifier for Arabic',trained_clf_random_forest1,x1_train, y1_train, x1_test, y1_test)

trained_clf_random_forest2 = RandomForestClassifier().fit(x2_train, y2_train)
get_accuracy('RandomForestClassifier for Turkish',trained_clf_random_forest2,x2_train, y2_train, x2_test, y2_test)

"""# **Naive Bayes**"""

from sklearn.naive_bayes import MultinomialNB
trained_clf_multinomial_nb1 = MultinomialNB().fit(x1_train, y1_train)
get_accuracy('MultinomialNB for Arabic',trained_clf_multinomial_nb1,x1_train, y1_train, x1_test, y1_test)

trained_clf_multinomial_nb2 = MultinomialNB().fit(x2_train, y2_train)
get_accuracy('MultinomialNB for Turkish',trained_clf_multinomial_nb2,x2_train, y2_train, x2_test, y2_test)

"""# **Linear SVM**"""

from sklearn.svm import LinearSVC
trained_clf_svc1 = LinearSVC().fit(x1_train, y1_train)
get_accuracy('LinearSVC for Arabic',trained_clf_svc1,x1_train, y1_train, x1_test, y1_test)

trained_clf_svc2 = LinearSVC().fit(x2_train, y2_train)
get_accuracy('LinearSVC for Turkish',trained_clf_svc2,x2_train, y2_train, x2_test, y2_test)

"""# **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
trained_clf_LogisticRegression1 = LogisticRegression().fit(x1_train, y1_train)
get_accuracy('LogisticRegression for Arabic',trained_clf_LogisticRegression1,x1_train, y1_train, x1_test, y1_test)

trained_clf_LogisticRegression2 = LogisticRegression().fit(x2_train, y2_train)
get_accuracy('LogisticRegression for Turkish',trained_clf_LogisticRegression2,x2_train, y2_train, x2_test, y2_test)

"""# **BERT**"""

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification

#Load the Tokenizer and Model
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)

# Select all Turkish reviews
turkish_reviews = df[df['language'] == 'Turkish']

# Randomly select 20,000 Arabic reviews
arabic_reviews = df[df['language'] == 'Arabic'].sample(n=3000, random_state=42)

# Combine both Turkish and Arabic reviews into one DataFrame
selected_reviews = pd.concat([turkish_reviews, arabic_reviews])

# Assuming 'rating' is the name of the column with ratings
selected_ratings = df.loc[selected_reviews.index, 'rating']

xx_train, xx_test, yy_train, yy_test = train_test_split(selected_reviews['review_description'], selected_ratings, test_size=0.2, random_state=42)

# Tokenize the text
train_encodings = tokenizer(xx_train.tolist(), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(xx_test.tolist(), truncation=True, padding=True, max_length=128)

# Convert to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    yy_train
)).shuffle(1000).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    yy_test
)).batch(16)

# Configure the Model for Training
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

# Train the Model
model.fit(train_dataset, epochs=3, validation_data=test_dataset)

#Evaluate the model’s performance
model.evaluate(test_dataset)

"""# **Testing**"""

def returnCleanText(text):

    #Remove Punctuation
    text = re.sub(r'[^\w\s]', ' ', text)  # This will replace all punctuation with spaces
    text = re.sub(r'_', '', text)  # This will remove underscores if present
    text = text.replace('؛',"", )

    #Remove StopWords
    text = " ".join([word for word in text.split() if word not in all_stopwords])

    # Transform emojis
    text = emojiTextTransform(text)

    #remove digits
    text = ''.join([word for word in text if not word.isdigit()])

    # lemmatize the reviews
    text = lemmer.lemmatize_text(text)
    text = " ".join(text)
    return text

testtext = returnCleanText("التطبيق للأسف اصبح سيء 💔 واسعارهم اعلى من اسعار المحلات مع زيادة الضريبة ورسوم الخدمة ع الفاتورة")

id2label = {0:"Negative",1:"Neutral",2:"Postive"}

import re

def is_arabic(text):
    # A regular expression that matches Arabic script characters
    arabic_pattern = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]')
    return arabic_pattern.search(text) is not None

def is_turkish(text):
    # A regular expression that matches common Turkish characters not found in English
    turkish_pattern = re.compile(r'[ğüşöçİĞÜŞÖÇ]')
    return turkish_pattern.search(text) is not None

# Example usage
print(is_arabic("هذا نص باللغة العربية"))  # Should return True
print(is_turkish("Bu uygulama çok çok kötü"))  # Should return True

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

def predict_sentiment(text, models, vectorizer, id2label):
    # Preprocess the text
    clean_text = returnCleanText(text)

    # Vectorize the text
    # Check the language of the text and use the appropriate vectorizer
    if is_arabic(clean_text):
      text_vector = arabic_vectorizer.transform([clean_text]).toarray()
       # Make predictions with each model
      predictions = {}
      for model_name, model in models_ar.items():
          predicted_label_num = model.predict(text_vector)[0]
          predicted_label = id2label[predicted_label_num]
          predictions[model_name] = predicted_label
    elif is_turkish(clean_text):
      text_vector = turkish_vectorizer.transform([clean_text]).toarray()
       # Make predictions with each model
      predictions = {}
      for model_name, model in models_tr.items():
          predicted_label_num = model.predict(text_vector)[0]
          predicted_label = id2label[predicted_label_num]
          predictions[model_name] = predicted_label
    else:
      raise ValueError("The language of the text is neither Arabic nor Turkish.")

    return predictions

# Dictionary of your trained models
models_ar = {
    'RandomForest': trained_clf_random_forest1,
    'MultinomialNB': trained_clf_multinomial_nb1,
    'LinearSVC': trained_clf_svc1,
    'LogisticRegression': trained_clf_LogisticRegression1,
}

models_tr = {
    'RandomForest': trained_clf_random_forest2,
    'MultinomialNB': trained_clf_multinomial_nb2,
    'LinearSVC': trained_clf_svc2,
    'LogisticRegression': trained_clf_LogisticRegression2,
}

# Example usage:
test_text_arabic1 = "التطبيق للأسف اصبح سيء 💔 واسعارهم اعلى من اسعار المحلات مع زيادة الضريبة ورسوم الخدمة ع الفاتورة"
test_text_arabic2 = "الخدمة كانت أبطأ من المتوقع والموظفين غير متعاونين."
test_text_arabic3 = "العرض كان مذهلاً وتجاوز كل التوقعات!"
test_text_arabic4 = "المنتج وصل في الوقت المحدد وكان مغلفًا جيدًا."
test_text_turkish1 = "Bu uygulama çok çok kötü"  # This app is very very bad. Negative
test_text_turkish2 = "Ürün zamanında geldi ve iyi paketlenmişti." # The product arrived on time and was well packaged. neutral
test_text_turkish3 = "Hizmet beklenenden daha yavaştı ve çalışanlar yardımcı olmadı." # The service was slower than expected and the staff was not helpful. negative
test_text_turkish4 = "Gösteri harikaydı ve tüm beklentileri aştı!" # The show was great and exceeded all expectations! positive

# Predict sentiment for Arabic text
predictions_arabic1 = predict_sentiment(test_text_arabic1, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic1}\nPredictions: {predictions_arabic1}\n")

predictions_arabic2 = predict_sentiment(test_text_arabic2, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic2}\nPredictions: {predictions_arabic2}\n")

predictions_arabic3 = predict_sentiment(test_text_arabic3, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic3}\nPredictions: {predictions_arabic3}\n")

predictions_arabic4 = predict_sentiment(test_text_arabic4, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic4}\nPredictions: {predictions_arabic4}\n")

# Predict sentiment for Turkish text
predictions_turkish1 = predict_sentiment(test_text_turkish1, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish1}\nPredictions: {predictions_turkish1}\n")

predictions_turkish2 = predict_sentiment(test_text_turkish2, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish2}\nPredictions: {predictions_turkish2}\n")

predictions_turkish3 = predict_sentiment(test_text_turkish3, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish3}\nPredictions: {predictions_turkish3}\n")

predictions_turkish4 = predict_sentiment(test_text_turkish4, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish4}\nPredictions: {predictions_turkish4}\n")

"""# **Testing BERT**"""

test1=["التطبيق للأسف اصبح سيء 💔 واسعارهم اعلى من اسعار المحلات مع زيادة الضريبة ورسوم الخدمة ع الفاتورة"]
test1_data_encodings = tokenizer(test1, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test1_input_ids = tf.constant(test1_data_encodings['input_ids'])  # Convert input encodings to tensors
test1_attention_mask = tf.constant(test1_data_encodings['attention_mask'])

predictions1 = model.predict([test1_input_ids, test1_attention_mask])
predicted_labels1 = tf.argmax(predictions1.logits, axis=1)
predicted_labels1 = int(predicted_labels1[0])

predicted_labels1

predicted_rate = {0:"Negative",1:"Neutral",2:"Postive"}

print(f"The review is : {test1}\n The rate is {predicted_rate[predicted_labels1]}")

test2=["صلحوا التطبيق ده بقى😡"]
test2_data_encodings = tokenizer(test2, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test2_input_ids = tf.constant(test2_data_encodings['input_ids'])  # Convert input encodings to tensors
test2_attention_mask = tf.constant(test2_data_encodings['attention_mask'])

predictions2 = model.predict([test2_input_ids, test2_attention_mask])
predicted_labels2 = tf.argmax(predictions2.logits, axis=1)
predicted_labels2 = int(predicted_labels2[0])

predicted_labels2

print(f"The review is : {test2}\n The rate is {predicted_rate[predicted_labels2]}")

test3=["التطبيق جامد اوي انا انبهرت"]
test3_data_encodings = tokenizer(test3, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test3_input_ids = tf.constant(test3_data_encodings['input_ids'])  # Convert input encodings to tensors
test3_attention_mask = tf.constant(test3_data_encodings['attention_mask'])

predictions3 = model.predict([test3_input_ids, test3_attention_mask])
predicted_labels3 = tf.argmax(predictions3.logits, axis=1)
predicted_labels3 = int(predicted_labels3[0])

predicted_labels3

print(f"The review is : {test3}\n The rate is {predicted_rate[predicted_labels3]}")

test4=[" تي خزي منتجات سيئة مش صاير منهن بكل"]
test4_data_encodings = tokenizer(test4, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test4_input_ids = tf.constant(test4_data_encodings['input_ids'])  # Convert input encodings to tensors
test4_attention_mask = tf.constant(test4_data_encodings['attention_mask'])

predictions4 = model.predict([test4_input_ids, test4_attention_mask])
predicted_labels4 = tf.argmax(predictions4.logits, axis=1)
predicted_labels4 = int(predicted_labels4[0])

predicted_labels4

print(f"The review is : {test4}\n The rate is {predicted_rate[predicted_labels4]}")

test5=["Ürün zamanında geldi ve iyi paketlenmişti."]
test5_data_encodings = tokenizer(test5, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test5_input_ids = tf.constant(test5_data_encodings['input_ids'])  # Convert input encodings to tensors
test5_attention_mask = tf.constant(test5_data_encodings['attention_mask'])

predictions5 = model.predict([test5_input_ids, test5_attention_mask])
predicted_labels5 = tf.argmax(predictions5.logits, axis=1)
predicted_labels5 = int(predicted_labels5[0])

predicted_labels5

print(f"The review is : {test5}\n The rate is {predicted_rate[predicted_labels5]}")

test6=["Hizmet beklenenden daha yavaştı ve çalışanlar yardımcı olmadı."]
test6_data_encodings = tokenizer(test6, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test6_input_ids = tf.constant(test6_data_encodings['input_ids'])  # Convert input encodings to tensors
test6_attention_mask = tf.constant(test6_data_encodings['attention_mask'])

predictions6 = model.predict([test6_input_ids, test6_attention_mask])
predicted_labels6 = tf.argmax(predictions6.logits, axis=1)
predicted_labels6 = int(predicted_labels6[0])

predicted_labels6

print(f"The review is : {test6}\n The rate is {predicted_rate[predicted_labels6]}")