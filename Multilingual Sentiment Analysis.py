# -*- coding: utf-8 -*-
"""Sentiment Analysis of Arabic and Turkish languages.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X0d-0m8o_5vQuTYfno2MSHom384kviFl
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --upgrade sklearn
# !pip install Arabic-Stopwords
# !pip install emoji
# !pip install Tashaphyne
# !pip install qalsadi
# !pip install langdetect
# !pip install --upgrade scikit-learn
# !pip install nltk
# !pip install pandas
# !pip install seaborn
# !pip install wordcloud
# !pip install arabic_reshaper
# !pip install python-bidi
# !pip install transformers
# !pip install tensorflow
# !pip install tf-keras
# !pip install scipy

"""# **Importing Libraries**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re

# Load your Arabic and Turkish datasets
# Replace 'arabic_data.csv' and 'turkish_data.csv' with the paths to your datasets
arabic_df = pd.read_csv('CompanyReviews.csv')
turkish_df = pd.read_csv('TurkishReviews.csv')

# Changing the Turkish comments' colmumn name as the Arabic comments column' name
turkish_df = turkish_df.rename(columns={'Yorum': 'review_description', 'Duygu': 'rating'})

# Add a new column to each dataset to identify the language
arabic_df['language'] = 'Arabic'
turkish_df['language'] = 'Turkish'

# Concatenate the two datasets
df = pd.concat([arabic_df, turkish_df])

df

"""# **Data Cleaning**"""

# Drop the first column
df = df.drop(df.columns[0], axis=1)

# Replace the rating values
df['rating'] = df['rating'].replace({1: 'positive', -1: 'negative', 0: 'neutral', 'Olumlu': 'positive' , 'Olumsuz': 'negative', 'Tarafsiz': 'neutral'})
df

# Replace NaN values in 'company' column with 'n/a'
df['company'] = df['company'].fillna('n/a')

#display the counts of each category
df['rating'].value_counts()

# Define the valid ratings
valid_ratings = ['negative', 'positive', 'neutral']

# Keep only the rows in your DataFrame where the 'rating' column is in valid_ratings
df = df[df['rating'].isin(valid_ratings)]

#display the counts of each category
df['rating'].value_counts()

# Group the DataFrame 'df' by the 'company' column and calculate the size of each group
# This will return a Series with the number of occurrences for each unique 'company' value
df.groupby('company').size()

# display the sum of null values in each column
df.isnull().sum()

# dropping rows that has null values in review_description column
df = df.dropna(subset=['review_description'])
df.isnull().sum()

# display the sum of duplicates in reviews
df.review_description.duplicated().sum()

# deleting rows that has the same reviews
df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)
df.review_description.duplicated().sum()

# Reset the index of the DataFrame
df = df.reset_index(drop=True)
df

"""## **Visualization**"""

# Count the number of occurrences of each rating
rating_counts = df['rating'].value_counts()

# Define a color for each rating
colors = ['green' if label == 'positive'
          else 'red' if label == 'negative'
          else 'gray'
          for label in rating_counts.index]

# Create the pie chart
plt.figure(figsize=(10,6))
plt.pie(rating_counts, labels = rating_counts.index, autopct='%1.1f%%', colors=colors)
plt.title('Ratings')
plt.show()

# Assuming 'language' is the name of the column with language labels
# and 'rating' is the name of the column with rating labels

# Filter the DataFrame for Turkish language reviews and count the ratings
turkish_ratings = df[df['language'] == 'Turkish']['rating'].value_counts()

# Define a color for each Turkish rating
turkish_colors = ['green' if label == 'positive'
                  else 'red' if label == 'negative'
                  else 'gray'
                  for label in turkish_ratings.index]

# Create the pie chart for Turkish ratings
plt.figure(figsize=(10,6))
plt.pie(turkish_ratings, labels = turkish_ratings.index, autopct='%1.1f%%', colors=turkish_colors)
plt.title('Turkish Ratings')
plt.show()

# Filter the DataFrame for Arabic language reviews and count the ratings
arabic_ratings = df[df['language'] == 'Arabic']['rating'].value_counts()

# Define a color for each Arabic rating
arabic_colors = ['green' if label == 'positive'
                 else 'red' if label == 'negative'
                 else 'gray'
                 for label in arabic_ratings.index]

# Create the pie chart for Arabic ratings
plt.figure(figsize=(10,6))
plt.pie(arabic_ratings, labels = arabic_ratings.index, autopct='%1.1f%%', colors=arabic_colors)
plt.title('Arabic Ratings')
plt.show()

# Get the value counts
rating_counts = df['rating'].value_counts()

# Define a color for each rating
colors = ['green' if label == 'positive'
          else 'red' if label == 'negative'
          else 'gray'
          for label in rating_counts.index]

# Create the bar plot
rating_counts.plot(kind='bar', color=colors)
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.title('Bar Plot of Ratings')
plt.show()

df.company.unique()

"""# **Data Preprocessing**"""

df.review_description=df.review_description.astype(str)
df.review_description=df.review_description.apply(lambda x:re.sub('[%s]' % re.escape("""!"#$%&'()*+,ØŒ-./:;<=>ØŸ?@[\]^_`{|}~"""), ' ', x))
df.review_description=df.review_description.apply(lambda x:x.replace('Ø›',"", ))
df

# Load the stop words with the correct encoding
with open('arabic.txt', 'r', encoding='utf-8') as f:
    arabic_stopwords = f.read().splitlines()

with open('turkish.txt', 'r', encoding='utf-8') as f:
    turkish_stopwords = f.read().splitlines()

#Some words needed to work with to will remove
for word in ['Ù„Ø§','Ù„ÙƒÙ†']:
    arabic_stopwords.remove(word)

# Combine the stop words into one list
all_stopwords = arabic_stopwords + turkish_stopwords

df.review_description = df.review_description.apply(lambda x:" ".join([word for word in x.split() if word not in all_stopwords]))

df

# Handling Emojis in reviews
arabic_emojis = {
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "ğŸ˜‚":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ’”":"Ù‚Ù„Ø¨ Ø­Ø²ÙŠÙ†",
    "ğŸ™‚":"ÙŠØ¨ØªØ³Ù…",
    "â¤ï¸":"Ø­Ø¨",
    "â¤":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø¨",
    "ğŸ˜­":"ÙŠØ¨ÙƒÙŠ",
    "ğŸ˜¢":"Ø­Ø²Ù†",
    "ğŸ˜”":"Ø­Ø²Ù†",
    "â™¥":"Ø­Ø¨",
    "ğŸ’œ":"Ø­Ø¨",
    "ğŸ˜…":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ™":"Ø­Ø²ÙŠÙ†",
    "ğŸ’•":"Ø­Ø¨",
    "ğŸ’™":"Ø­Ø¨",
    "ğŸ˜":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜Š":"Ø³Ø¹Ø§Ø¯Ø©",
    "ğŸ‘":"ÙŠØµÙÙ‚",
    "ğŸ‘Œ":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜´":"ÙŠÙ†Ø§Ù…",
    "ğŸ˜€":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜Œ":"Ø­Ø²ÙŠÙ†",
    "ğŸŒ¹":"ÙˆØ±Ø¯Ø©",
    "ğŸ™ˆ":"Ø­Ø¨",
    "ğŸ˜„":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜":"Ù…Ø­Ø§ÙŠØ¯",
    "âœŒ":"Ù…Ù†ØªØµØ±",
    "âœ¨":"Ù†Ø¬Ù…Ù‡",
    "ğŸ¤”":"ØªÙÙƒÙŠØ±",
    "ğŸ˜":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ˜’":"ÙŠØ³ØªÙ‡Ø²Ø¡",
    "ğŸ™„":"Ù…Ù„Ù„",
    "ğŸ˜•":"Ø¹ØµØ¨ÙŠØ©",
    "ğŸ˜ƒ":"ÙŠØ¶Ø­Ùƒ",
    "ğŸŒ¸":"ÙˆØ±Ø¯Ø©",
    "ğŸ˜“":"Ø­Ø²Ù†",
    "ğŸ’":"Ø­Ø¨",
    "ğŸ’—":"Ø­Ø¨",
    "ğŸ˜‘":"Ù…Ù†Ø²Ø¹Ø¬",
    "ğŸ˜":"Ø«Ù‚Ø©",
    "ğŸ’›":"Ø­Ø¨",
    "ğŸ˜©":"Ø­Ø²ÙŠÙ†",
    "ğŸ’ª":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ‘":"Ù…ÙˆØ§ÙÙ‚",
    "ğŸ™ğŸ»":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ˜³":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ‘ğŸ¼":"ØªØµÙÙŠÙ‚",
    "ğŸŒš":"ØµÙ…Øª",
    "ğŸ’š":"Ø­Ø¨",
    "ğŸ™":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "ğŸ’˜":"Ø­Ø¨",
    "â˜º":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜¶":"Ù…ØµØ¯ÙˆÙ…",
    "âœŒï¸":"Ù…Ø±Ø­",
    "âœ‹ğŸ»":"ØªÙˆÙ‚Ù",
    "ğŸ˜‰":"ØºÙ…Ø²Ø©",
    "ğŸŒ·":"Ø­Ø¨",
    "ğŸ™ƒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜«":"Ø­Ø²ÙŠÙ†",
    "ğŸ˜¨":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’Ÿ":"Ø­Ø¨",
    "ğŸ˜ª":"Ø­Ø²Ù†",
    "ğŸ˜†":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜£":"Ø§Ø³ØªÙŠØ§Ø¡",
    "â˜ºï¸":"Ø­Ø¨",
    "ğŸ˜±":"ÙƒØ§Ø±Ø«Ø©",
    "ğŸ˜":"ÙŠØ¶Ø­Ùƒ",
    "ğŸ˜–":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸƒğŸ¼":"ÙŠØ¬Ø±ÙŠ",
    "ğŸ˜¡":"ØºØ¶Ø¨",
    "ğŸš¶":"ÙŠØ³ÙŠØ±",
    "ğŸ¤•":"Ù…Ø±Ø¶",
    "â€¼ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘ŒğŸ»":"Ø§Ø­Ø³Ù†Øª",
    "â£":"Ø­Ø¨",
    "ğŸ™Š":"Ù…ØµØ¯ÙˆÙ…",
    "ğŸ’ƒ":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ’ƒğŸ¼":"Ø³Ø¹Ø§Ø¯Ø© Ù…Ø±Ø­",
    "ğŸ˜œ":"Ù…Ø±Ø­",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜Ÿ":"Ø§Ø³ØªÙŠØ§Ø¡",
    "ğŸ’–":"Ø­Ø¨",
    "ğŸ˜¥":"Ø­Ø²Ù†",
    "ğŸ’":"Ø§Ù„Ù…Ø§Ø¸",
    "ğŸ˜·":"ÙˆØ¨Ø§Ø¡ Ù…Ø±Ø¶",
    "âš ï¸" :"ØªØ­Ø°ÙŠØ±",
    "ğŸ¤—" : "Ø§Ø­ØªÙˆØ§Ø¡",
    "âœ–ï¸": "ØºÙ„Ø·",
    "ğŸ‘‘" : "ØªØ§Ø¬",
    "âœ”ï¸" : "ØµØ­",
    "ğŸ’Œ": "Ù‚Ù„Ø¨",
    "ğŸ˜²" : "Ù…Ù†Ø¯Ù‡Ø´",
    "ğŸš«" : "Ø®Ø·Ø§",
    "ğŸ‘ğŸ»" : "Ø¨Ø±Ø§ÙÙˆ",
    "ğŸ‘ğŸ»": "ØªÙ…Ø§Ù…",
    "âœŒğŸ¼": "Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù„Ù†ØµØ±",
    "ğŸŒ":"Ù…Ø¨ØªØ³Ù…",
    "ğŸ˜§" : "Ù‚Ù„Ù‚ Ùˆ ØµØ¯Ù…Ø©",
    "â—ï¸" :"ØªØ¹Ø¬Ø¨",
    "ğŸ‘ğŸ½":"Ø§ÙŠØ¯ÙŠ Ù…ÙØªÙˆØ­Ù‡",
    "ğŸ‘ŒğŸ½": "Ø¨Ø§Ù„Ø¸Ø¨Ø·",
    "â‰ï¸" : "Ø§Ø³ØªÙ†ÙƒØ§Ø±",
    "ğŸ€":    "ÙˆØ±Ø¯Ù‡",
    "ğŸ’µ":  "ÙÙ„ÙˆØ³",
    "ğŸ˜‹":  "Ø¬Ø§Ø¦Ø¹",
    "ğŸ˜›":  "ÙŠØºÙŠØ¸",
    "ğŸ˜ ":  "ØºØ§Ø¶Ø¨",
    "âŒ":"Ø±ÙØ¶",
    "ğŸ‘ŒğŸ¼":"Ø§Ø­Ø³Ù†Øª",
    "ğŸ˜®":"ØµØ¯Ù…Ø©",
    "ğŸ˜¦":"Ù‚Ù„Ù‚",
    "ğŸ¤":"Ø­Ø²Ù†",
    "ğŸ’«":"Ù…Ø±Ø­",
    "ğŸ’":"Ø­Ø¨",
    "â¤ï¸":"Ø­Ø¨",
    "ğŸ™ğŸ¼":"Ø±Ø¬Ø§Ø¡ Ø·Ù„Ø¨",
    "â€¼":"ØªØ¹Ø¬Ø¨",
    "â™¥ï¸":"Ø­Ø¨",
    "ğŸ’©":"Ù…Ø¹ØªØ±Ø¶",
    "ğŸŒŸ":"Ù†Ø¬Ù…Ø©",
    "ğŸ‘ŠğŸ¼":"Ø¶Ø±Ø¨",
    "ğŸ‘Š":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜š":"Ø­Ø¨",
    "ğŸ‘ğŸ»":"Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ",
    "ğŸ‘ŠğŸ½":"Ø¶Ø±Ø¨Ø©",
    "ğŸ˜™":"Ø­Ø¨",
    "ğŸ‘ğŸ½":"ÙŠØµÙÙ‚",
    "ğŸ’ªğŸ»":"Ø¹Ø¶Ù„Ø§Øª",
    "ğŸ”¥":"Ø­Ø±ÙŠÙ‚",
    "ğŸ˜¬":"Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©",
    "ğŸ‘ŠğŸ¿":"ÙŠØ¶Ø±Ø¨",
    "âœ‹ğŸ¼":"ÙƒÙ Ø§ÙŠØ¯",
    "â˜ ï¸":"ÙˆØ¬Ù‡ Ù…Ø±Ø¹Ø¨",
    "ğŸ‰":"ÙŠÙ‡Ù†Ø¦",
    "ğŸ˜¿":"ÙˆØ¬Ù‡ Ø­Ø²ÙŠÙ†",
    "â˜¹ï¸":"ÙˆØ¬Ù‡ ÙŠØ§Ø¦Ø³",
    "ğŸ˜˜" :"Ø­Ø¨",
    "ğŸ˜°" :"Ø®ÙˆÙ Ùˆ Ø­Ø²Ù†",
    "ğŸŒ¼":"ÙˆØ±Ø¯Ù‡",
    "ğŸ’‹":  "Ø¨ÙˆØ³Ù‡",
    "â£ï¸":"Ø­Ø¨",
    "ğŸ˜‡":"Ø¯Ø§ÙŠØ®",
    "ğŸ˜ˆ":"Ø±Ø¹Ø¨",
    "â—ï¸":"ØªØ¹Ø¬Ø¨",
    "ğŸ‘":"ØºÙŠØ± Ù…ÙˆØ§ÙÙ‚",
    "ğŸ˜¯":"Ù…ØªÙØ§Ø¬Ø£",
    "ğŸ˜»":"Ø§Ø¹Ø¬Ø§Ø¨",
    "ğŸ’ªğŸ½":"Ù‚ÙˆÙ‡",
    "ğŸ˜¤":"ÙˆØ¬Ù‡ Ø¹Ø§Ø¨Ø³",
    "ğŸ˜¹":"Ø¶Ø­Ùƒ",
    "ğŸ’“":"Ø­Ø¨",
    "ğŸ‘»":"Ø±Ø¹Ø¨",
    "â":"Ø®Ø·Ø¡",
    "ğŸ¤®":"Ø­Ø²Ù†",
    }

turkish_emojis = {
    "ğŸ™‚": "GÃ¼lÃ¼mseme",
    "ğŸ˜‚": "Kahkaha",
    "ğŸ’”": "KÄ±rÄ±k Kalp",
    "â¤ï¸": "AÅŸk",
    "ğŸ˜": "AÅŸÄ±k OlmuÅŸ",
    "ğŸ˜­": "AÄŸlama",
    "ğŸ˜¢": "ÃœzgÃ¼n",
    "ğŸ˜”": "DÃ¼ÅŸÃ¼nceli",
    "â™¥": "AÅŸk",
    "ğŸ’œ": "AÅŸk",
    "ğŸ˜…": "Terli GÃ¼lÃ¼mseme",
    "ğŸ™": "ÃœzgÃ¼n",
    "ğŸ’•": "AÅŸk",
    "ğŸ’™": "AÅŸk",
    "ğŸ˜": "ÃœzgÃ¼n",
    "ğŸ˜Š": "Mutluluk",
    "ğŸ‘": "AlkÄ±ÅŸ",
    "ğŸ‘Œ": "MÃ¼kemmel",
    "ğŸ˜´": "Uyku",
    "ğŸ˜€": "Kahkaha",
    "ğŸ˜Œ": "RahatlamÄ±ÅŸ",
    "ğŸŒ¹": "GÃ¼l",
    "ğŸ™ˆ": "UtangaÃ§",
    "ğŸ˜„": "Kahkaha",
    "ğŸ˜": "NÃ¶tr",
    "âœŒ": "Zafer",
    "âœ¨": "ParÄ±ltÄ±",
    "ğŸ¤”": "DÃ¼ÅŸÃ¼nce",
    "ğŸ˜": "KÃ¼stah",
    "ğŸ˜’": "RahatsÄ±z",
    "ğŸ™„": "SÄ±kÄ±lmÄ±ÅŸ",
    "ğŸ˜•": "Hayal KÄ±rÄ±klÄ±ÄŸÄ±",
    "ğŸ˜ƒ": "Kahkaha",
    "ğŸŒ¸": "Ã‡iÃ§ek",
    "ğŸ˜“": "ÃœzgÃ¼n",
    "ğŸ’": "AÅŸk",
    "ğŸ’—": "AÅŸk",
    "ğŸ˜‘": "RahatsÄ±z",
    "ğŸ˜": "HavalÄ±",
    "ğŸ’›": "AÅŸk",
    "ğŸ˜©": "Yorgun",
    "ğŸ’ª": "GÃ¼Ã§",
    "ğŸ‘": "Onay",
    "ğŸ™ğŸ»": "Dua",
    "ğŸ˜³": "ÅaÅŸkÄ±n",
    "ğŸ‘ğŸ¼": "AlkÄ±ÅŸ",
    "ğŸŒš": "Gizemli",
    "ğŸ’š": "AÅŸk",
    "ğŸ™": "Dua",
    "ğŸ’˜": "AÅŸk",
    "â˜º": "GÃ¼lÃ¼mseme",
    "ğŸ˜¶": "ÅaÅŸkÄ±n",
    "âœŒï¸": "EÄŸlence",
    "âœ‹ğŸ»": "Dur",
    "ğŸ˜‰": "GÃ¶z KÄ±rpma",
    "ğŸŒ·": "Ã‡iÃ§ek",
    "ğŸ™ƒ": "Ters GÃ¼lÃ¼mseme",
    "ğŸ˜«": "Yorgun",
    "ğŸ˜¨": "KorkmuÅŸ",
    "ğŸ’Ÿ": "AÅŸk",
    "ğŸ˜ª": "Uykulu",
    "ğŸ˜†": "Kahkaha",
    "ğŸ˜£": "ÃœzgÃ¼n",
    "â˜ºï¸": "GÃ¼lÃ¼mseme",
    "ğŸ˜±": "Panik",
    "ğŸ˜": "Kahkaha",
    "ğŸ˜–": "RahatsÄ±z",
    "ğŸƒğŸ¼": "KoÅŸu",
    "ğŸ˜¡": "Ã–fkeli",
    "ğŸš¶": "YÃ¼rÃ¼me",
    "ğŸ¤•": "YaralÄ±",
    "â€¼ï¸": "Ãœnlem",
    "ğŸ‘ŒğŸ»": "MÃ¼kemmel",
    "â£": "AÅŸk",
    "ğŸ™Š": "Sessiz",
    "ğŸ’ƒ": "Dans",
    "ğŸ’ƒğŸ¼": "Dans",
    "ğŸ˜œ": "ÅakacÄ±",
    "ğŸ‘Š": "Yumruk",
    "ğŸ˜Ÿ": "EndiÅŸeli",
    "ğŸ’–": "AÅŸk",
    "ğŸ˜¥": "ÃœzgÃ¼n",
    "ğŸ’": "Elmas",
    "ğŸ˜·": "Hasta",
    "âš ï¸": "UyarÄ±",
    "ğŸ¤—": "SarÄ±lmak",
    "âœ–ï¸": "YanlÄ±ÅŸ",
    "ğŸ‘‘": "TaÃ§",
    "âœ”ï¸": "DoÄŸru",
    "ğŸ’Œ": "Mektup",
    "ğŸ˜²": "ÅaÅŸkÄ±n",
    "ğŸš«": "Yasak",
    "ğŸ‘ğŸ»": "AlkÄ±ÅŸ",
    "ğŸ‘ğŸ»": "Onay",
    "âœŒğŸ¼": "Zafer Ä°ÅŸareti",
    "ğŸŒ": "GÃ¼lÃ¼mseme",
    "ğŸ˜§": "EndiÅŸeli",
    "â—ï¸": "Ãœnlem",
    "ğŸ‘ğŸ½": "AÃ§Ä±k Eller",
    "ğŸ‘ŒğŸ½": "Tamam",
    "â‰ï¸": "Soru Ãœnlem",
    "ğŸ€": "Kurdele",
    "ğŸ’µ": "Para",
    "ğŸ˜‹": "Lezzetli",
    "ğŸ˜›": "Dil Ã‡Ä±karma",
    "ğŸ˜ ": "KÄ±zgÄ±n",
    "âŒ": "YanlÄ±ÅŸ",
    "ğŸ‘ŒğŸ¼": "MÃ¼kemmel",
    "ğŸ˜®": "ÅaÅŸkÄ±n",
    "ğŸ˜¦": "EndiÅŸeli",
    "ğŸ¤": "KuÅŸ",
    "ğŸ’«": "BaÅŸ DÃ¶nmesi",
    "ğŸ’": "Hediye Kalbi",
    "â¤ï¸": "AÅŸk",
    "ğŸ™ğŸ¼": "Dua",
    "â€¼": "Ãœnlem",
    "â™¥ï¸": "AÅŸk",
    "ğŸ’©": "Bok",
    "ğŸŒŸ": "YÄ±ldÄ±z",
    "ğŸ‘ŠğŸ¼": "Yumruk",
    "ğŸ˜š": "Ã–pÃ¼cÃ¼k",
    "ğŸ‘ğŸ»": "BeÄŸenmeme",
    "ğŸ‘ŠğŸ½": "Yumruk",
    "ğŸ˜™": "Ã–pÃ¼cÃ¼k",
    "ğŸ‘ğŸ½": "AlkÄ±ÅŸ",
    "ğŸ’ªğŸ»": "GÃ¼Ã§",
    "ğŸ”¥": "AteÅŸ",
    "ğŸ˜¬": "SÄ±kÄ±ntÄ±",
    "ğŸ‘ŠğŸ¿": "Yumruk",
    "âœ‹ğŸ¼": "El",
    "â˜ ï¸": "KafatasÄ±",
    "ğŸ‰": "Kutlama",
    "ğŸ˜¿": "AÄŸlayan Kedi",
    "â˜¹ï¸": "ÃœzgÃ¼n YÃ¼z",
    "ğŸ˜˜": "Ã–pÃ¼cÃ¼k",
    "ğŸ˜°": "EndiÅŸeli",
    "ğŸŒ¼": "Ã‡iÃ§ek",
    "ğŸ’‹": "Ã–pÃ¼cÃ¼k",
    "â£ï¸": "AÅŸk",
    "ğŸ˜‡": "Masum",
    "ğŸ˜ˆ": "Åeytan",
    "ğŸ‘": "BeÄŸenmeme",
    "ğŸ˜¯": "ÅaÅŸkÄ±n",
    "ğŸ˜»": "Kalp GÃ¶zlÃ¼ Kedi",
    "ğŸ’ªğŸ½": "GÃ¼Ã§",
    "ğŸ˜¤": "BuÄŸulu Nefes",
    "ğŸ˜¹": "GÃ¼len Kedi",
    "ğŸ’“": "Kalp AtÄ±ÅŸÄ±",
    "ğŸ‘»": "Hayalet",
    "â": "YanlÄ±ÅŸ",
    "ğŸ¤®": "Kusma",
}


emotions_to_emoji = {
    ":)" : "ğŸ™‚",
    ":(" : "ğŸ™",
    "xD" : "ğŸ˜†",
    ":=(": "ğŸ˜­",
    ":'(": "ğŸ˜¢",
    ":'â€‘(": "ğŸ˜¢",
    "XD" : "ğŸ˜‚",
    ":D" : "ğŸ™‚",
    "â™¬" : "ğŸµ",
    "â™¡" : "â¤ï¸",
    "â˜»"  : "ğŸ™‚",
}

from langdetect import detect, LangDetectException
import emoji

def emoticons_to_emoji(text):
    for emoticon, emoji in emotions_to_emoji.items():
        text = text.replace(emoticon, emoji)
    return text

def checkemojie(text, lang):
    emojistext=[]
    for char in text:
        if any(emoji.distinct_emoji_list(char)):
            if lang == 'ar':
                if char in arabic_emojis.keys():
                    emojistext.append(arabic_emojis[emoji.distinct_emoji_list(char)[0]])
            else:
                if char in turkish_emojis.keys():
                    emojistext.append(turkish_emojis[emoji.distinct_emoji_list(char)[0]])
    return " ".join(emojistext)

def emojiTextTransform(text):
    text = emoticons_to_emoji(text)  # Transform emoticons to emojis
    cleantext = re.sub(r'[^\w\s]', '', text)
    try:
        lang = detect(cleantext)  # Detect the language of the text
    except LangDetectException:
        lang = 'unknown'
    return cleantext + " " + checkemojie(text, lang)




emojiTextTransform(df.review_description[71])

emojiTextTransform(df.review_description[38999])

df.review_description=df.review_description.apply(lambda x:emojiTextTransform(x))
df.review_description[71]

df.review_description.duplicated().sum()

df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)

# remove digits
df.review_description=df.review_description.apply(lambda x:''.join([word for word in x if not word.isdigit()]))

#applying lemmatizer
import qalsadi.lemmatizer
lemmer = qalsadi.lemmatizer.Lemmatizer()

df.review_description=df.review_description.apply(lambda x:lemmer.lemmatize_text(x))
df

df.review_description=df.review_description.apply(lambda x:" ".join(x))
df

df.review_description.duplicated().sum()

df.drop(df[df.review_description.duplicated() == True].index, axis = 0, inplace = True)

df.review_description.duplicated().sum()

df.review_description.isnull().sum()

import arabic_reshaper
from bidi.algorithm import get_display

# Function to reshape and apply bidi support to Arabic text
def reshape_arabic_text(text):
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    return bidi_text

# Apply the reshaping function to the Arabic text only
arabic_reviews = ' '.join(df[df['language'] == 'Arabic']['review_description'])
reshaped_arabic_reviews = reshape_arabic_text(arabic_reviews)

from wordcloud import WordCloud
# Generate the word cloud using an Arabic font
wordcloud = WordCloud(
    width = 800, height = 800,
    background_color ='white',
    stopwords = all_stopwords,
    min_font_size = 10,
    font_path=r'C:\Users\Tameem\Arial Unicode MS Regular.ttf'  # Specify the path to an Arabic font
).generate(reshaped_arabic_reviews)

# Display the word cloud using matplotlib
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
plt.show()

from scipy.sparse import hstack
import numpy as np
# Feature Extraction
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming 'df' is your DataFrame and it has a 'language' column to distinguish between Turkish and Arabic
turkish_texts = df[df['language'] == 'Turkish']['review_description']
arabic_texts = df[df['language'] == 'Arabic']['review_description']

# Vectorize Turkish texts
turkish_vectorizer = TfidfVectorizer()
turkish_features = turkish_vectorizer.fit_transform(turkish_texts)

# Optionally, vectorize Arabic texts
arabic_vectorizer = TfidfVectorizer(max_features=10000)
arabic_features = arabic_vectorizer.fit_transform(arabic_texts)

# Now, 'turkish_features' contains the TF-IDF features for Turkish texts,
# and 'arabic_features' contains the TF-IDF features for Arabic texts.


# for Turkish and Arabic texts respectively
turkish_feature_names = turkish_vectorizer.get_feature_names_out()
arabic_feature_names = arabic_vectorizer.get_feature_names_out()

def label_to_num(label):
    return {'negative': -1, 'neutral': 0, 'positive': 1}[label]

df['rating'] = df['rating'].apply(label_to_num)  # Convert string labels to numerical values

# remap the class labels in the â€˜ratingâ€™ column of the DataFrame, it maps -1 to 0, 0 to 1, and 1 to 2
def mappingclasses(classx):
    return {-1:0,0:1,1:2}[classx]
df['rating']=df['rating'].apply(lambda x:mappingclasses(x))

# Create a DataFrame with the feature names as columns
X1 = pd.DataFrame(arabic_features.toarray(),columns=arabic_feature_names)
X2= pd.DataFrame(turkish_features.toarray(),columns=turkish_feature_names)

Y=df.rating

from sklearn.model_selection import train_test_split
# Assuming 'df' has a 'rating' column with labels and a 'language' column to distinguish languages
arabic_labels = df[df['language'] == 'Arabic']['rating']
turkish_labels = df[df['language'] == 'Turkish']['rating']

# Split the Arabic data
x1_train, x1_test, y1_train, y1_test = train_test_split(X1, arabic_labels, random_state=42, test_size=0.20, shuffle=True)

# Split the Turkish data
x2_train, x2_test, y2_train, y2_test = train_test_split(X2, turkish_labels, random_state=42, test_size=0.20, shuffle=True)

"""# **Modelling**"""

import numpy as np
from sklearn import metrics
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, classification_report , roc_curve, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score,make_scorer,mean_squared_error
from sklearn.model_selection import cross_val_score, GridSearchCV


def get_accuracy(name, trained_model , x_train, y_train, x_test, y_test):
    tree_predict = trained_model.predict(x_test)
    print("Testing accuracy   :",metrics.accuracy_score(y_test, tree_predict)*100 , "%")
    tree_predict1 = trained_model.predict(x_train)
    print("Training accuracy  :",metrics.accuracy_score(y_train, tree_predict1)*100 ,"%")
    print("precision : ",precision_score(y_test, tree_predict,average='micro'))
    print("recall    : ",recall_score(y_test, tree_predict,average='micro'))
    print("f1_score  : ",f1_score(y_test, tree_predict,average='micro'))

    cf1 = confusion_matrix(y_test,tree_predict)
    sns.heatmap(cf1,annot=True,fmt = '.0f')
    plt.xlabel('prediction')
    plt.ylabel('Actual')
    plt.title(name+ ' Confusion Matrix')
    plt.show()

    print("                      Traing Classification Report                      ")
    print(classification_report(y_train,  trained_model.predict(x_train)))
    print("                      Testing Classification Report                      ")
    print(classification_report(y_test,  trained_model.predict(x_test)))

"""# **Random Forrest**"""

from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
trained_clf_random_forest1 = RandomForestClassifier().fit(x1_train, y1_train)
get_accuracy('RandomForestClassifier for Arabic',trained_clf_random_forest1,x1_train, y1_train, x1_test, y1_test)

trained_clf_random_forest2 = RandomForestClassifier().fit(x2_train, y2_train)
get_accuracy('RandomForestClassifier for Turkish',trained_clf_random_forest2,x2_train, y2_train, x2_test, y2_test)

"""# **Naive Bayes**"""

from sklearn.naive_bayes import MultinomialNB
trained_clf_multinomial_nb1 = MultinomialNB().fit(x1_train, y1_train)
get_accuracy('MultinomialNB for Arabic',trained_clf_multinomial_nb1,x1_train, y1_train, x1_test, y1_test)

trained_clf_multinomial_nb2 = MultinomialNB().fit(x2_train, y2_train)
get_accuracy('MultinomialNB for Turkish',trained_clf_multinomial_nb2,x2_train, y2_train, x2_test, y2_test)

"""# **Linear SVM**"""

from sklearn.svm import LinearSVC
trained_clf_svc1 = LinearSVC().fit(x1_train, y1_train)
get_accuracy('LinearSVC for Arabic',trained_clf_svc1,x1_train, y1_train, x1_test, y1_test)

trained_clf_svc2 = LinearSVC().fit(x2_train, y2_train)
get_accuracy('LinearSVC for Turkish',trained_clf_svc2,x2_train, y2_train, x2_test, y2_test)

"""# **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
trained_clf_LogisticRegression1 = LogisticRegression().fit(x1_train, y1_train)
get_accuracy('LogisticRegression for Arabic',trained_clf_LogisticRegression1,x1_train, y1_train, x1_test, y1_test)

trained_clf_LogisticRegression2 = LogisticRegression().fit(x2_train, y2_train)
get_accuracy('LogisticRegression for Turkish',trained_clf_LogisticRegression2,x2_train, y2_train, x2_test, y2_test)

"""# **BERT**"""

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification

#Load the Tokenizer and Model
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)

# Select all Turkish reviews
turkish_reviews = df[df['language'] == 'Turkish']

# Randomly select 20,000 Arabic reviews
arabic_reviews = df[df['language'] == 'Arabic'].sample(n=3000, random_state=42)

# Combine both Turkish and Arabic reviews into one DataFrame
selected_reviews = pd.concat([turkish_reviews, arabic_reviews])

# Assuming 'rating' is the name of the column with ratings
selected_ratings = df.loc[selected_reviews.index, 'rating']

xx_train, xx_test, yy_train, yy_test = train_test_split(selected_reviews['review_description'], selected_ratings, test_size=0.2, random_state=42)

# Tokenize the text
train_encodings = tokenizer(xx_train.tolist(), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(xx_test.tolist(), truncation=True, padding=True, max_length=128)

# Convert to TensorFlow Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    yy_train
)).shuffle(1000).batch(16)

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    yy_test
)).batch(16)

# Configure the Model for Training
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

# Train the Model
model.fit(train_dataset, epochs=3, validation_data=test_dataset)

#Evaluate the modelâ€™s performance
model.evaluate(test_dataset)

"""# **Testing**"""

def returnCleanText(text):

    #Remove Punctuation
    text = re.sub(r'[^\w\s]', ' ', text)  # This will replace all punctuation with spaces
    text = re.sub(r'_', '', text)  # This will remove underscores if present
    text = text.replace('Ø›',"", )

    #Remove StopWords
    text = " ".join([word for word in text.split() if word not in all_stopwords])

    # Transform emojis
    text = emojiTextTransform(text)

    #remove digits
    text = ''.join([word for word in text if not word.isdigit()])

    # lemmatize the reviews
    text = lemmer.lemmatize_text(text)
    text = " ".join(text)
    return text

testtext = returnCleanText("Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù Ø§ØµØ¨Ø­ Ø³ÙŠØ¡ ğŸ’” ÙˆØ§Ø³Ø¹Ø§Ø±Ù‡Ù… Ø§Ø¹Ù„Ù‰ Ù…Ù† Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ø§Øª Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¶Ø±ÙŠØ¨Ø© ÙˆØ±Ø³ÙˆÙ… Ø§Ù„Ø®Ø¯Ù…Ø© Ø¹ Ø§Ù„ÙØ§ØªÙˆØ±Ø©")

id2label = {0:"Negative",1:"Neutral",2:"Postive"}

import re

def is_arabic(text):
    # A regular expression that matches Arabic script characters
    arabic_pattern = re.compile(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]')
    return arabic_pattern.search(text) is not None

def is_turkish(text):
    # A regular expression that matches common Turkish characters not found in English
    turkish_pattern = re.compile(r'[ÄŸÃ¼ÅŸÃ¶Ã§Ä°ÄÃœÅÃ–Ã‡]')
    return turkish_pattern.search(text) is not None

# Example usage
print(is_arabic("Ù‡Ø°Ø§ Ù†Øµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"))  # Should return True
print(is_turkish("Bu uygulama Ã§ok Ã§ok kÃ¶tÃ¼"))  # Should return True

import warnings

# Ignore user warnings
warnings.filterwarnings('ignore', category=UserWarning)

def predict_sentiment(text, models, vectorizer, id2label):
    # Preprocess the text
    clean_text = returnCleanText(text)

    # Vectorize the text
    # Check the language of the text and use the appropriate vectorizer
    if is_arabic(clean_text):
      text_vector = arabic_vectorizer.transform([clean_text]).toarray()
       # Make predictions with each model
      predictions = {}
      for model_name, model in models_ar.items():
          predicted_label_num = model.predict(text_vector)[0]
          predicted_label = id2label[predicted_label_num]
          predictions[model_name] = predicted_label
    elif is_turkish(clean_text):
      text_vector = turkish_vectorizer.transform([clean_text]).toarray()
       # Make predictions with each model
      predictions = {}
      for model_name, model in models_tr.items():
          predicted_label_num = model.predict(text_vector)[0]
          predicted_label = id2label[predicted_label_num]
          predictions[model_name] = predicted_label
    else:
      raise ValueError("The language of the text is neither Arabic nor Turkish.")

    return predictions

# Dictionary of your trained models
models_ar = {
    'RandomForest': trained_clf_random_forest1,
    'MultinomialNB': trained_clf_multinomial_nb1,
    'LinearSVC': trained_clf_svc1,
    'LogisticRegression': trained_clf_LogisticRegression1,
}

models_tr = {
    'RandomForest': trained_clf_random_forest2,
    'MultinomialNB': trained_clf_multinomial_nb2,
    'LinearSVC': trained_clf_svc2,
    'LogisticRegression': trained_clf_LogisticRegression2,
}

# Example usage:
test_text_arabic1 = "Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù Ø§ØµØ¨Ø­ Ø³ÙŠØ¡ ğŸ’” ÙˆØ§Ø³Ø¹Ø§Ø±Ù‡Ù… Ø§Ø¹Ù„Ù‰ Ù…Ù† Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ø§Øª Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¶Ø±ÙŠØ¨Ø© ÙˆØ±Ø³ÙˆÙ… Ø§Ù„Ø®Ø¯Ù…Ø© Ø¹ Ø§Ù„ÙØ§ØªÙˆØ±Ø©"
test_text_arabic2 = "Ø§Ù„Ø®Ø¯Ù…Ø© ÙƒØ§Ù†Øª Ø£Ø¨Ø·Ø£ Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ ÙˆØ§Ù„Ù…ÙˆØ¸ÙÙŠÙ† ØºÙŠØ± Ù…ØªØ¹Ø§ÙˆÙ†ÙŠÙ†."
test_text_arabic3 = "Ø§Ù„Ø¹Ø±Ø¶ ÙƒØ§Ù† Ù…Ø°Ù‡Ù„Ø§Ù‹ ÙˆØªØ¬Ø§ÙˆØ² ÙƒÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª!"
test_text_arabic4 = "Ø§Ù„Ù…Ù†ØªØ¬ ÙˆØµÙ„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙˆÙƒØ§Ù† Ù…ØºÙ„ÙÙ‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§."
test_text_turkish1 = "Bu uygulama Ã§ok Ã§ok kÃ¶tÃ¼"  # This app is very very bad. Negative
test_text_turkish2 = "ÃœrÃ¼n zamanÄ±nda geldi ve iyi paketlenmiÅŸti." # The product arrived on time and was well packaged. neutral
test_text_turkish3 = "Hizmet beklenenden daha yavaÅŸtÄ± ve Ã§alÄ±ÅŸanlar yardÄ±mcÄ± olmadÄ±." # The service was slower than expected and the staff was not helpful. negative
test_text_turkish4 = "GÃ¶steri harikaydÄ± ve tÃ¼m beklentileri aÅŸtÄ±!" # The show was great and exceeded all expectations! positive

# Predict sentiment for Arabic text
predictions_arabic1 = predict_sentiment(test_text_arabic1, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic1}\nPredictions: {predictions_arabic1}\n")

predictions_arabic2 = predict_sentiment(test_text_arabic2, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic2}\nPredictions: {predictions_arabic2}\n")

predictions_arabic3 = predict_sentiment(test_text_arabic3, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic3}\nPredictions: {predictions_arabic3}\n")

predictions_arabic4 = predict_sentiment(test_text_arabic4, models_ar, arabic_vectorizer, id2label)
print(f"Arabic Text: {test_text_arabic4}\nPredictions: {predictions_arabic4}\n")

# Predict sentiment for Turkish text
predictions_turkish1 = predict_sentiment(test_text_turkish1, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish1}\nPredictions: {predictions_turkish1}\n")

predictions_turkish2 = predict_sentiment(test_text_turkish2, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish2}\nPredictions: {predictions_turkish2}\n")

predictions_turkish3 = predict_sentiment(test_text_turkish3, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish3}\nPredictions: {predictions_turkish3}\n")

predictions_turkish4 = predict_sentiment(test_text_turkish4, models_tr, turkish_vectorizer, id2label)
print(f"Turkish Text: {test_text_turkish4}\nPredictions: {predictions_turkish4}\n")

"""# **Testing BERT**"""

test1=["Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„Ù„Ø£Ø³Ù Ø§ØµØ¨Ø­ Ø³ÙŠØ¡ ğŸ’” ÙˆØ§Ø³Ø¹Ø§Ø±Ù‡Ù… Ø§Ø¹Ù„Ù‰ Ù…Ù† Ø§Ø³Ø¹Ø§Ø± Ø§Ù„Ù…Ø­Ù„Ø§Øª Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¶Ø±ÙŠØ¨Ø© ÙˆØ±Ø³ÙˆÙ… Ø§Ù„Ø®Ø¯Ù…Ø© Ø¹ Ø§Ù„ÙØ§ØªÙˆØ±Ø©"]
test1_data_encodings = tokenizer(test1, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test1_input_ids = tf.constant(test1_data_encodings['input_ids'])  # Convert input encodings to tensors
test1_attention_mask = tf.constant(test1_data_encodings['attention_mask'])

predictions1 = model.predict([test1_input_ids, test1_attention_mask])
predicted_labels1 = tf.argmax(predictions1.logits, axis=1)
predicted_labels1 = int(predicted_labels1[0])

predicted_labels1

predicted_rate = {0:"Negative",1:"Neutral",2:"Postive"}

print(f"The review is : {test1}\n The rate is {predicted_rate[predicted_labels1]}")

test2=["ØµÙ„Ø­ÙˆØ§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ù‡ Ø¨Ù‚Ù‰ğŸ˜¡"]
test2_data_encodings = tokenizer(test2, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test2_input_ids = tf.constant(test2_data_encodings['input_ids'])  # Convert input encodings to tensors
test2_attention_mask = tf.constant(test2_data_encodings['attention_mask'])

predictions2 = model.predict([test2_input_ids, test2_attention_mask])
predicted_labels2 = tf.argmax(predictions2.logits, axis=1)
predicted_labels2 = int(predicted_labels2[0])

predicted_labels2

print(f"The review is : {test2}\n The rate is {predicted_rate[predicted_labels2]}")

test3=["Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¬Ø§Ù…Ø¯ Ø§ÙˆÙŠ Ø§Ù†Ø§ Ø§Ù†Ø¨Ù‡Ø±Øª"]
test3_data_encodings = tokenizer(test3, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test3_input_ids = tf.constant(test3_data_encodings['input_ids'])  # Convert input encodings to tensors
test3_attention_mask = tf.constant(test3_data_encodings['attention_mask'])

predictions3 = model.predict([test3_input_ids, test3_attention_mask])
predicted_labels3 = tf.argmax(predictions3.logits, axis=1)
predicted_labels3 = int(predicted_labels3[0])

predicted_labels3

print(f"The review is : {test3}\n The rate is {predicted_rate[predicted_labels3]}")

test4=[" ØªÙŠ Ø®Ø²ÙŠ Ù…Ù†ØªØ¬Ø§Øª Ø³ÙŠØ¦Ø© Ù…Ø´ ØµØ§ÙŠØ± Ù…Ù†Ù‡Ù† Ø¨ÙƒÙ„"]
test4_data_encodings = tokenizer(test4, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test4_input_ids = tf.constant(test4_data_encodings['input_ids'])  # Convert input encodings to tensors
test4_attention_mask = tf.constant(test4_data_encodings['attention_mask'])

predictions4 = model.predict([test4_input_ids, test4_attention_mask])
predicted_labels4 = tf.argmax(predictions4.logits, axis=1)
predicted_labels4 = int(predicted_labels4[0])

predicted_labels4

print(f"The review is : {test4}\n The rate is {predicted_rate[predicted_labels4]}")

test5=["ÃœrÃ¼n zamanÄ±nda geldi ve iyi paketlenmiÅŸti."]
test5_data_encodings = tokenizer(test5, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test5_input_ids = tf.constant(test5_data_encodings['input_ids'])  # Convert input encodings to tensors
test5_attention_mask = tf.constant(test5_data_encodings['attention_mask'])

predictions5 = model.predict([test5_input_ids, test5_attention_mask])
predicted_labels5 = tf.argmax(predictions5.logits, axis=1)
predicted_labels5 = int(predicted_labels5[0])

predicted_labels5

print(f"The review is : {test5}\n The rate is {predicted_rate[predicted_labels5]}")

test6=["Hizmet beklenenden daha yavaÅŸtÄ± ve Ã§alÄ±ÅŸanlar yardÄ±mcÄ± olmadÄ±."]
test6_data_encodings = tokenizer(test6, truncation=True,padding="max_length", max_length=32) # Tokenize the data  texts
test6_input_ids = tf.constant(test6_data_encodings['input_ids'])  # Convert input encodings to tensors
test6_attention_mask = tf.constant(test6_data_encodings['attention_mask'])

predictions6 = model.predict([test6_input_ids, test6_attention_mask])
predicted_labels6 = tf.argmax(predictions6.logits, axis=1)
predicted_labels6 = int(predicted_labels6[0])

predicted_labels6

print(f"The review is : {test6}\n The rate is {predicted_rate[predicted_labels6]}")